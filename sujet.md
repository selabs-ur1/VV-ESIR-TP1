# Practical Session #1: Introduction

1. Find in news sources a general public article reporting the discovery of a software bug. Describe the bug. If possible, say whether the bug is local or global and describe the failure that manifested its presence. Explain the repercussions of the bug for clients/consumers and the company or entity behind the faulty program. Speculate whether, in your opinion, testing the right scenario would have helped to discover the fault.

En 2021, Tesla a rappelé environ 12000 véhicules suite à un bug dans le logiciel de conduite autonome. Ce bug fut découvert suite à plusieurs retour d'utilisaturs expliquant que le logiciel du véhicule avait faussement identifié une collision en approche résultant en l'activation du système automatique de freinage d'urgence. Ce bug a été introduit par une mise à jour problématique du logiciel de conduite autonome entraînant une défaillance de communication entre deux puces informatiques. L'entreprise a décidé de procéder à un rappel volontaire pour éviter d'éventuels conflits avec la National Highway Traffic Safety Administration (NHTSA) des États-Unis, qui examinait de près la manière dont Tesla gérait les mises à jour logicielles et les rappels de sécurité. Du point de vue des tests logiciels, des tests rigoureux dans des scénarios réels ou des simulations auraient pu potentiellement identifier la défaillance de communication entre les puces avant le déploiement de la mise à jour logicielle. Étant donné que le problème était lié à une interaction spécifique entre le matériel et le logiciel, des tests d'intégration complets auraient pu détecter le bug, évitant ainsi la nécessité d'un rappel. Cependant, l'identification et la résolution rapides du problème après sa manifestation suggèrent que les protocoles de surveillance et de réponse de Tesla étaient efficaces, même si les tests initiaux n'ont pas pris en compte ce scénario particulier.

2. Apache Commons projects are known for the quality of their code and development practices. They use dedicated issue tracking systems to discuss and follow the evolution of bugs and new features. The following link https://issues.apache.org/jira/projects/COLLECTIONS/issues/COLLECTIONS-794?filter=doneissues points to the issues considered as solved for the Apache Commons Collections project. Among those issues find one that corresponds to a bug that has been solved. Classify the bug as local or global. Explain the bug and the solution. Did the contributors of the project add new tests to ensure that the bug is detected if it reappears in the future?

The BloomFilter: Converting Double to Int bug is global.
The issue with the BloomFilter in Apache Commons Collections arises from a method that converts a precise value from the filter's shape into an imprecise one, leading to potential inaccuracies. Additionally, there is no argument validation on the filter’s shape, which can cause merge errors if the filters have different numbers of bits. To ensure proper functionality, the merge operation should be symmetric, meaning the smaller filter should be merged into the larger one. Afterward, the `estimateN` method should be called on the larger filter to maintain this symmetry. The recommended solution is to remove the problematic method and instead include a detailed explanation in the class Javadoc. This explanation would guide users on how to estimate the value of `N`, as well as determine the size of the union and intersection when working with other filters.
The contributors removed the problematic method and added documentation, they did not add new tests to detect the bug in the future.

3. Netflix is famous, among other things we love, for the popularization of *Chaos Engineering*, a fault-tolerance verification technique. The company has implemented protocols to test their entire system in production by simulating faults such as a server shutdown. During these experiments they evaluate the system's capabilities of delivering content under different conditions. The technique was described in [a paper](https://arxiv.org/ftp/arxiv/papers/1702/1702.05843.pdf) published in 2016. Read the paper and briefly explain what are the concrete experiments they perform, what are the requirements for these experiments, what are the variables they observe and what are the main results they obtained. Is Netflix the only company performing these experiments? Speculate how these experiments could be carried in other organizations in terms of the kind of experiment that could be performed and the system variables to observe during the experiments.

Nous pouvons citer plusieurs expériences concrètes menées par Netflix dans le domaine de *Chaos Engineering* :

Chaos Monkey : Cet outil éteint aléatoirement des instances de services dans l'environnement de production de Netflix. L'objectif est de s'assurer que le système peut gérer la défaillance de composants individuels sans impact significatif sur la fourniture des services.
Chaos Gorilla : Cette expérience simule la panne d'une zone de disponibilité entière d'Amazon Web Services (AWS). Il s'agit d'un test plus étendu par rapport à Chaos Monkey et aide à évaluer la capacité du système à gérer des pannes à grande échelle.
Latency Monkey : Il introduit des délais artificiels dans les communications entre services pour tester la manière dont le système gère des scénarios de haute latence et pour s'assurer qu'il n'y a pas de dégradation des services.
Doctor Monkey : Cet outil simule des instances ou des services qui ne fonctionnent pas correctement sans être complètement hors service, afin de tester la gestion et l'isolation des composants partiellement défaillants.
Conformity Monkey : Il vérifie si les instances en production respectent les meilleures pratiques ou les principes architecturaux définis par Netflix, garantissant ainsi la conformité dans l'ensemble de l'environnement.

Pour que ces expériences de *Chaos Engineering* soient menées à bien, il faut tout de même s'assurer de certains prérequis :

Environnement de Production : Les expériences sont réalisées dans l'environnement de production réel pour refléter le plus précisément possible les conditions réelles.
Exécution Contrôlée : Les tests sont effectués de manière contrôlée pour minimiser l'impact sur les clients et éviter des pannes généralisées.
Observabilité et Surveillance : Un système de surveillance et d'observabilité robuste est essentiel pour capturer les données sur la manière dont le système se comporte lors de pannes.
Mécanismes de Repli : Des mécanismes de repli appropriés doivent être en place au cas où les expériences causeraient des effets secondaires inattendus ou trop de perturbations de service.

Les variables observées lors de ces expériences sont les suivantes : 

Disponibilité et Temps de Fonctionnement des Services : Capacité du système à maintenir sa disponibilité lorsque certains composants ou des zones entières sont arrêtés.
Taux d'Erreurs et Latence : Fréquence des erreurs et leur impact sur les temps de réponse lorsque des pannes sont introduites.
Débit du Système : Capacité globale du système à traiter des requêtes pendant des défaillances partielles ou des délais.
Temps de Récupération : Temps nécessaire au système pour se rétablir et se stabiliser après une panne simulée.

Les résultats de ces expériences se traduisent par une résilience améliorée permettant de gérer différents types de pannes sans impacter considérablement l'expérience utilisateur. Netflix a développé des systèmes de récupération automatisé lors de tests de *Chaos Engineering* réduisant ainsi grandement la nécessité d'intervention manuelle. Cependant, Netflix n'est pas la seule entreprise à mener des expériences de *Chaos Engineering*. D'autres organisations comme Google, Amazon, LinkedIn et Microsoft utilisent également des principes de *Chaos Engineering* pour tester la résilience de leurs systèmes.

Dans d'autres organisations, des expériences similaires pourraient être menées en fonction de leurs besoins opérationnels et de leur configuration d'infrastructure:

Tests de Partitionnement Réseau : Simuler des partitions réseau pour observer comment le système gère des ruptures de communication entre services distribués.
Simulation de Panne de Base de Données : Introduire des pannes dans les bases de données pour tester comment l'application se comporte lorsque les données sont inaccessibles ou disponibles de manière incohérente.
Privation de Ressources : Limiter les ressources CPU, mémoire ou disque pour tester comment les services se dégradent et priorisent les charges de travail critiques.
Simulations de Panne de Fournisseur Cloud : Pour les organisations fortement dépendantes des fournisseurs de cloud, simuler des pannes de services cloud pour tester les stratégies de basculement multi-cloud ou hybride.

4. [WebAssembly](https://webassembly.org/) has become the fourth official language supported by web browsers. The language was born from a joint effort of the major players in the Web. Its creators presented their design decisions and the formal specification in [a scientific paper](https://people.mpi-sws.org/~rossberg/papers/Haas,%20Rossberg,%20Schuff,%20Titzer,%20Gohman,%20Wagner,%20Zakai,%20Bastien,%20Holman%20-%20Bringing%20the%20Web%20up%20to%20Speed%20with%20WebAssembly.pdf) published in 2018. The goal of the language is to be a low level, safe and portable compilation target for the Web and other embedding environments. The authors say that it is the first industrial strength language designed with formal semantics from the start. This evidences the feasibility of constructive approaches in this area. Read the paper and explain what are the main advantages of having a formal specification for WebAssembly. In your opinion, does this mean that WebAssembly implementations should not be tested? 

The main advantages of having a formal specification for WebAssembly, as outlined in the paper, include several key benefits. First, it enhances safety and security by ensuring that WebAssembly code can be executed without compromising user data or system state. Second, it supports portability, guaranteeing that WebAssembly code will run consistently across various hardware and software environments. Third, the formal specification facilitates efficiency by allowing for streamlined validation and compilation processes, which lead to faster execution times. Additionally, designing with formal semantics from the outset contributes to a notably clean and well-structured language. Despite these advantages, it's important to note that a formal specification does not negate the need for testing. Implementations must still be thoroughly tested to ensure compliance with the specification and to identify any potential bugs or performance issues.

5.  Shortly after the appearance of WebAssembly another paper proposed a mechanized specification of the language using Isabelle. The paper can be consulted here: https://www.cl.cam.ac.uk/~caw77/papers/mechanising-and-verifying-the-webassembly-specification.pdf. This mechanized specification complements the first formalization attempt from the paper. According to the author of this second paper, what are the main advantages of the mechanized specification? Did it help improving the original formal specification of the language? What other artifacts were derived from this mechanized specification? How did the author verify the specification? Does this new specification removes the need for testing?

The paper outlines several key points regarding the mechanized specification of WebAssembly. First, it highlights the advantages of this approach, noting that it provides a verified executable interpreter and type checker, thereby ensuring the soundness of the WebAssembly type system. Additionally, this mechanized specification has uncovered issues in the official specification, which have subsequently influenced its development. Second, the paper discusses improvements to the formal specification, as the mechanized proof identified deficiencies in the original specification, leading to necessary corrections and enhancements in the type system’s soundness. Third, it mentions that the work produced verified executable tools, such as an interpreter and type checker, which, however, require integration with external parsers and linkers to function as standalone programs. The verification methods employed include using Isabelle for soundness proofs, complemented by conformance tests and fuzzing experiments. Lastly, while the mechanized specification offers strong guarantees, it does not eliminate the need for testing, as it still depends on an untrusted interface for parsing and linking.

6.  

## Answers
